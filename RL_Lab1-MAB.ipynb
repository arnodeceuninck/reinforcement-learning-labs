{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name:**\n",
    "\n",
    "**Student ID:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Multi-Armed Bandits\n",
    "\n",
    "### Instructions: **TODO** tags\n",
    "In this lab session, we will be implementing the multi-armed bandit framework, along with several algorithms studied in the course. This lab aims to understand their differences in design and performance.\n",
    "\n",
    "Please *read* and *run* the notebook chronologically, and fill in the **TODO**s as you encounter them.\n",
    "* <span style=\"color:blue\"> Blue **TODOs** </span> means you have to implement the TODOs in the code.\n",
    "* <span style=\"color:red\"> Red **TODOs** </span> means you have to submit an explanation (of graph/results).\n",
    "\n",
    "\n",
    "At each section, <span style=\"color:green\"> (xx points) </span> indicates the number of points of the entire section (labs are graded out of 10).\n",
    "### Libraries\n",
    "- We will be using [numpy](https://numpy.org/doc/stable/user/absolute_beginners.html), the main library for linear algebra in Python, for virtually all of our mathematical operations in all the labs. It enables efficient operations on arrays of scalars. If you are unfamiliar with numpy, please look into the documentation or tutorials (navigator searches work well!). In this lab, you will for example need to [initialize arrays](https://numpy.org/doc/stable/reference/generated/numpy.full.html) or [generate random numbers](https://docs.scipy.org/doc/numpy-1.15.1/reference/routines.random.html) from miscellaneous distributions.\n",
    "\n",
    "- We will be using [matplotlib](https://matplotlib.org/stable/index.html) for plotting. You normally won't need to look into it, but feel free to do so if you need to display fancier graphs.\n",
    "\n",
    "\n",
    "## 1. <span style=\"color:green\"> (3 points) </span> Multi-armed Bandit Setting\n",
    "First we will implement the agent-bandit interactions. <br>\n",
    "At each step, the *Agent* performs an *action* using its *policy*. This is formalized as the `act` method. <br>\n",
    "Given said action, the *Bandit* (slot machine) provides a *reward* as feedback. This is formalized as the `pull` method. <br>\n",
    "The agent *learns* from that feedback (adapts its policy). This is formalized as the `learn` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Abstract Agent object\n",
    "The Agent is `reset` at the start of a run. <br>\n",
    "During a run, it can `act` to pull a lever, and `learn` from an `action`-`reward` pair.<br>\n",
    "Please read the doc and function signatures; you don't have to write anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit_Agent(object):\n",
    "    \"\"\"\n",
    "    Abstract Agent to solve a Bandit problem.\n",
    "\n",
    "    Contains the methods learn() and act() for the base life cycle of an agent.\n",
    "    The reset() method reinitializes the agent.\n",
    "    The minimum requirment to instantiate a child class of Bandit_Agent\n",
    "    is that it implements the act() method.\n",
    "    \"\"\"\n",
    "    def __init__(self, k:int, **kwargs):\n",
    "        \"\"\"\n",
    "        Simply stores the number of arms of the Bandit problem.\n",
    "        The __init__() method handles hyperparameters.\n",
    "        Parameters\n",
    "        ----------\n",
    "        k: positive int\n",
    "            Number of arms of the Bandit problem.\n",
    "        kwargs: dictionary\n",
    "            Additional parameters, ignored.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinitializes the agent to 0 knowledge, good as new.\n",
    "\n",
    "        No inputs or outputs.\n",
    "        The reset() method handles variables.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self, a:int, r:float):\n",
    "        \"\"\"\n",
    "        Learning method. The agent learns that action a yielded reward r.\n",
    "        Parameters\n",
    "        ----------\n",
    "        a: positive int < k\n",
    "            Action that yielded the received reward r.\n",
    "        r: float\n",
    "            Reward for having performed action a.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def act(self) -> int:\n",
    "        \"\"\"\n",
    "        Agent's method to select a lever (or Bandit) to pull.\n",
    "        Returns\n",
    "        -------\n",
    "        a : positive int < k\n",
    "            The action the agent chose to perform.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Calling method act() in Abstract class Bandit_Agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. <span style=\"color:blue\"> **TODO** </span>: Random Agent\n",
    "Now we implement the *Random* Agent as a baseline. <br> \n",
    "The Random agent does not learn, it picks a random action at all steps. <br>\n",
    "<span style=\"color:blue\"> **TODO** </span> Implement the  act method of the Random Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Agent(Bandit_Agent):\n",
    "    \"\"\"\n",
    "    This agent doesn't learn, just acts purely randomly.\n",
    "    \"\"\"\n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        Random action selection.\n",
    "        Returns\n",
    "        -------\n",
    "        a : positive int < k\n",
    "            A randomly selected action.\n",
    "        \"\"\"\n",
    "        # TODO, using the numpy random function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. <span style=\"color:blue\"> **TODO** </span> KBandit object\n",
    "In this lab our k-armed Bandit is going to be a set of k Gaussian distributions. We **also** select the **means** from a **standard normalized Gaussian N(0,1)**. Note that in the general case, the distributions of each rewards can be anything and don't have to be within the same distribution family. \n",
    "\n",
    "The KBandit (k-armed Bandit) is `reset` at the start of a run. <br>\n",
    "During a run, the agent chooses a lever, which the bandit `pull`s to output a reward.<br>\n",
    "Please read the doc and function signatures; you don't have to write anything here.\n",
    "\n",
    "<span style=\"color:blue\"> **TODO** </span>: fill in the TODOs in the code (1 in `reset` method; 1 in `pull` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-f56b29f59305>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-6-f56b29f59305>\"\u001B[1;36m, line \u001B[1;32m19\u001B[0m\n\u001B[1;33m    self.distributions = # TODO: list of k Gaussians, i.e. (mu,std=1) pairs, with mu sampled from N(0,1)\u001B[0m\n\u001B[1;37m                                                                                                        ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class KBandit(object):\n",
    "    \"\"\" Set of k Gaussian distributions as reward functions. \"\"\"\n",
    "    def __init__(self, k, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiates the k-armed bandit, with a number of arms, and initializes\n",
    "        the set of distributions to new standardized Gaussians in a list.\n",
    "        The reset() method is supposedly called from outside.\n",
    "        The means are chosen randomly in a standard normalized Gaussian N(0,1).\n",
    "        Parameters\n",
    "        ----------\n",
    "        k: positive int\n",
    "            Number of arms of the problem.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets each of the k bandits. \"\"\"\n",
    "        self.distributions = # TODO: list of k Gaussians, i.e. (mu,std=1) pairs, with mu sampled from N(0,1)\n",
    "        \n",
    "    def best_action(self):\n",
    "        \"\"\" A function only for our analysis; the agents do not know this. \"\"\"\n",
    "        return np.argmax([distrib[0] for distrib in self.distributions]) # for plotting purposes\n",
    "        \n",
    "    def pull(self, action:int) -> float:\n",
    "        \"\"\"\n",
    "        Pulls the lever corresponding to the action. \n",
    "        In other words, samples a reward from the corresponding distribution.\n",
    "        Returns the reward.\n",
    "        Parameters\n",
    "        ----------\n",
    "        action: positive int < k\n",
    "            Lever to pull.\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            Reward for pulling this lever.\n",
    "        \"\"\"\n",
    "        # TODO: pull a lever to sample a reward given an action, return the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. <span style=\"color:blue\"> **TODO** </span> Running the Agent-Bandit interactions\n",
    "Now we can implement the loop of interactions of the Agent with the Bandit.\n",
    "To visualize our results, we want to store the reward obtained, as well as whether the action performed was optimal.\n",
    "\n",
    "<span style=\"color:blue\"> **TODO** </span>: fill in the TODOs in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bandit(agent, kbandit, max_steps) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "    Runs a Bandit problem once. The kbandit and agent are reinitializated,\n",
    "    then for max_steps, we run the bandit-agent interactions with learning.\n",
    "    We return the performance of the agent, i.e. the rewards along the way,\n",
    "    as well as a boolean array of the best action being performed.\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent: Bandit_Agent\n",
    "        An instance of a Bandit_Agent to solve the problem.\n",
    "    kbandit: KBandit\n",
    "        k-armed bandit problem, i.e. k slot machines with a reward distribution.\n",
    "    max_steps: positive int\n",
    "        Number of steps to run the problem.\n",
    "    Returns\n",
    "    -------\n",
    "    perf, best_action : np.array, np.array\n",
    "        Arrays of size max_steps containing all\n",
    "            - perf: rewards obtained by the agent during the run.\n",
    "            - best_action: boolean array, whether the agent did the best action.\n",
    "    \"\"\"\n",
    "    # TODO: prepare the agent and bandit for the run.\n",
    "    perf = np.empty(max_steps)\n",
    "    best_action = np.empty(max_steps)\n",
    "    for step in range(max_steps):\n",
    "        action = # TODO: agent acts\n",
    "        reward = # TODO: pull an arm of the bandit\n",
    "        # TODO: learn that this action gave this reward\n",
    "\n",
    "        # Plotting. Nothing to do here.\n",
    "        perf[step] = reward\n",
    "        best_action[step] = int(action == kbandit.best_action())\n",
    "\n",
    "    return perf, best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Plotting the results: Random Agent performance\n",
    "Since these experiments involve a lot of stochasticity, we do multiple runs in order to extract average behavior.<br>\n",
    "**Run** the following cells to visualize the Random agent performance. <br>\n",
    "You don't have anything to write in the next code cells. Please do read through them to understand what we're doing.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_bandits(n_runs, **kwargs) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "    Runs multiple independent bandit problems; outputs the mean of the results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent, kbandit, n_runs: see run_bandit\n",
    "        See function run_bandit\n",
    "    Returns\n",
    "    -------\n",
    "    ret : expected_type\n",
    "        description\n",
    "    \"\"\"\n",
    "    perfs = []\n",
    "    best_actions = []\n",
    "    for run in range(n_runs):\n",
    "        perf, best_action = run_bandit(**kwargs)\n",
    "        perfs.append(perf)\n",
    "        best_actions.append(best_action)\n",
    "\n",
    "    return np.mean(perfs,axis=0), np.mean(best_actions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(l, suptitle, title, xlabel, ylabel,\n",
    "              labels=None, interval_yaxis=None):\n",
    "    \"\"\" Simply saves a plot with multiple usual arguments.\"\"\"\n",
    "    if labels is None:\n",
    "        plt.plot(l)\n",
    "    else:\n",
    "        for perf, label in zip(l, labels):\n",
    "            plt.plot(smooth_fast(perf), label=label)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.suptitle(suptitle, fontsize=14, fontweight='bold')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    if interval_yaxis is not None:\n",
    "        new_y1, new_y2 = interval_yaxis\n",
    "        x1,x2,y1,y2 = plt.axis()\n",
    "        plt.axis((x1,x2,new_y1,new_y2))\n",
    "\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    \n",
    "def smooth_fast(y, box_pts=5):\n",
    "    n = len(y)\n",
    "    ws = 2*box_pts +1\n",
    "    y_smooth = []\n",
    "    for i,yi in enumerate(y):\n",
    "        if i < box_pts:\n",
    "            y_smooth.append(None)\n",
    "        elif i == box_pts:\n",
    "            y_smooth.append(np.mean(y[i-box_pts:i+box_pts+1]))\n",
    "        elif i < n - box_pts:\n",
    "            y_smooth.append(y_smooth[i-1] + (y[i + box_pts] - y[i - box_pts - 1])/ws)\n",
    "        elif i < n:\n",
    "            y_smooth.append(None)\n",
    "\n",
    "    return y_smooth\n",
    "\n",
    "def action_plot(l, suptitle, title, labels=None):\n",
    "    make_plot(l, suptitle, title, 'Steps', 'Best action proportion', labels, interval_yaxis=[0,1])\n",
    "\n",
    "def perf_plot(l, suptitle, title, labels=None):\n",
    "    make_plot(l, suptitle, title, 'Steps', 'Average Reward', labels, interval_yaxis=None)\n",
    "\n",
    "def dict_string(d):\n",
    "    \"\"\" Turns a dictionary to a single readable string \"\"\"\n",
    "    s = \"\"\n",
    "    for key, value in d.items():\n",
    "        s += \"{}:{}, \".format(key, value)\n",
    "    return s[:-2] # erase final comma and space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETERS AND AGENTS ==================================================\n",
    "config = {\n",
    "    'k': 10\n",
    "}\n",
    "\n",
    "n_runs = 1000\n",
    "max_steps = 1000 # time budget per run\n",
    "agent = Random_Agent(**config)\n",
    "kbandit = KBandit(**config)\n",
    "\n",
    "perfs, best_actions = run_multiple_bandits(n_runs, agent=agent, kbandit=kbandit, max_steps=max_steps)\n",
    "labels = ['Random']\n",
    "suptitle = 'Random Agent on Gaussian k-armed-Bandit'\n",
    "title = dict_string(config)\n",
    "\n",
    "action_plot ([best_actions], suptitle, title, labels)\n",
    "perf_plot   ([perfs],  suptitle, title, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. <span style=\"color:red\"> **TODO** </span> Please explain your results in a paragraph in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <span style=\"color:green\"> (5 points) </span> Bandit Algorithms\n",
    "Now we are going to implement learning Agents in order to select the best action/lever/arm to pull to maximize the obtained reward.\n",
    "\n",
    "**Note** that a lot of these agents will use an `argmax`, however numpy returns the *first* argmax, while we want to break ties randomly, since there is no reason to prefer an action than another, and this might harm exploration. Please use the following argmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_argmax(x):\n",
    "    return np.random.choice(np.where(x == x.max())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. <span style=\"color:blue\"> **TODO** </span> Epsilon-Greedy, Sample Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsGreedy_SampleAverage:\n",
    "    # TODO: implement this class following the Agent formalism above.\n",
    "    # This class uses Sample Averages to estimate q; others are non stationary.\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. <span style=\"color:blue\"> **TODO** </span> Epsilon-Greedy, Weighted Average\n",
    "Please use `lr` for the learning rate; it is passed as input to the `__init__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsGreedy_WeightedAverage:\n",
    "    # TODO: implement this class following the formalism above.\n",
    "    # Non stationary agent with q estimating and eps-greedy action selection.\n",
    "    \n",
    "    # PLEASE USE THE LR HYPERPARAMETER FOR THE LEARNING RATE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. <span style=\"color:blue\"> **TODO** </span> Optimistic Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticGreedy:\n",
    "    # TODO: implement this class following the formalism above.\n",
    "    # Same as above but with optimistic starting values.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. <span style=\"color:blue\"> **TODO** </span> UCB\n",
    "UCB works with either Sample or Weighted Averages. Please use Weighted Averages. Please use `lr` for the learning rate; it is passed as input to the `__init__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    # TODO: implement this class following the formalism above.\n",
    "    \n",
    "    # PLEASE USE THE LR HYPERPARAMETER FOR THE LEARNING RATE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. <span style=\"color:blue\"> **TODO** </span> Gradient Bandit\n",
    "Please use `alpha` for the learning rate; it is passed as input to the `__init__` method.\n",
    "\n",
    "Below are some functions that you might find useful: the softmax function used to extract a policy from the preferences; and a surprisingly faster version of `np.random.choice`, which is trying to do too complicated things for our simple purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't have anything to do in this cell. These methods will be helpful for the Gradient Bandit Agent.\n",
    "def softmax(x):\n",
    "    \"\"\" Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def my_random_choice(v, p=None):\n",
    "    \"\"\" Faster version of the np.random.choice function with probabilities \"\"\"\n",
    "    if p is None:\n",
    "        return v[np.random.randint(len(v))]\n",
    "    # else (general case)\n",
    "    assert (abs(sum(p)-1.)<1e-6), \"Invalid probability vector p, sum={}\".format(sum(p))\n",
    "    r = np.random.rand()\n",
    "    i = 0\n",
    "    s = p[i]\n",
    "    while s < r:\n",
    "        i += 1\n",
    "        s += p[i]\n",
    "\n",
    "    if type(v) is int:\n",
    "        assert len(p) == v, \"Int doesn't match proba length: {} != {}\".format(v, len(p))\n",
    "        return i\n",
    "    else:\n",
    "        assert len(v) == len(p), \"Incorrect entry lengths v,p: {} != {}\".format(len(v), len(p))\n",
    "        return v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient_Bandit:\n",
    "    # TODO: implement this class following the formalism above.\n",
    "    # If you want this to run fast, use the my_random_choice function defined above,\n",
    "    # instead of np.random.choice to sample from the softmax.\n",
    "    # You can also find the softmax function above.\n",
    "    \n",
    "    # PLEASE USE THE ALPHA HYPERPARAMETER FOR THE LEARNING RATE\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Plotting the results: All Agent performances.\n",
    "We now compare all agents.\n",
    "**Run** the following cells to visualize all the Agents' performances at once. <br>\n",
    "You don't have anything to write in the next code cells. Please do read through them to understand what we're doing.<br>\n",
    "\n",
    "*If don't want to test all agents at once during your implementation, comment out the rest from the list of `agents`.* <br> \n",
    "**You have to submit the combined plot with all the Agents; please don't make us run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_agents(agents, **kwargs):\n",
    "    \"\"\"\n",
    "    Launches run_multiple_bandits for a list of agents.\n",
    "    Outputs a list of the returns.\n",
    "    Parameters\n",
    "    ----------\n",
    "    agents : list of agents\n",
    "        instantiated agents on which to perform the runs on.\n",
    "    **kwargs: dictionary\n",
    "        Inputs for the function run_multiple_bandits\n",
    "    Returns\n",
    "    -------\n",
    "    ret : list\n",
    "        list of the outputs from the run_multiple_bandits function for each\n",
    "        agent from the agents list.\n",
    "    \"\"\"\n",
    "    perfs = []\n",
    "    best_actions = []\n",
    "    for agent in agents:\n",
    "        print(agent.__class__.__name__)\n",
    "        kwargs['agent'] = agent\n",
    "        perf, best_action = run_multiple_bandits(**kwargs)\n",
    "        perfs.append(perf)\n",
    "        best_actions.append(best_action)\n",
    "\n",
    "    return perfs, best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETERS AND AGENTS ==================================================\n",
    "config = {\n",
    "    'k': 10,\n",
    "    'lr': 0.1,\n",
    "    'alpha': 0.25,\n",
    "    'eps': 0.1,\n",
    "    'c': 0.5,\n",
    "    'q0': 1\n",
    "}\n",
    "\n",
    "n_runs = 1000\n",
    "max_steps = 1000 # time budget per run\n",
    "agents = [\n",
    "    Random_Agent(**config),\n",
    "    EpsGreedy_WeightedAverage(**config),\n",
    "    EpsGreedy_SampleAverage(**config),\n",
    "    OptimisticGreedy(**config),\n",
    "    UCB(**config),\n",
    "    Gradient_Bandit(**config)\n",
    "]\n",
    "\n",
    "kbandit = KBandit(**config)\n",
    "\n",
    "perfs, best_actions = run_multiple_agents(agents=agents, n_runs=n_runs, kbandit=kbandit, max_steps=max_steps)\n",
    "labels = [agent.__class__.__name__ for agent in agents]\n",
    "suptitle = 'Agents Performances in k-armed-Bandit'\n",
    "title = dict_string(config)\n",
    "\n",
    "action_plot (best_actions, suptitle, title, labels)\n",
    "perf_plot   (perfs,  suptitle, title, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **TODO** </span> Please explain your results in a paragraph in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  <span style=\"color:green\"> (2 points) </span> Varied Bandit distributions\n",
    "\n",
    "So far, we have worked with a bandit of k Normal Gaussian distributions. However, in the general case, the reward distributions for each arm could be anything, and certainly do not need to be so well-behaved. For example, an arm triggering the roll of a dice would mean a uniform discrete distribution. Medical treatments results can often be interpreted as Bernouilli distribution outputs (success with some probability). Taking a single individual from a set of multiple populations can be associatedwith sampling from a Gaussian mixture. Playing an actual slot machine would mean a high probability for small negative reward (cost of playing), and rare high rewards with the right symbols combination, resulting in a particular discrete distribution.\n",
    "\n",
    "Can our algorithms still adapt to these wilder distributions? Let's try it out! \n",
    "\n",
    "### 3.1. <span style=\"color:blue\"> **TODO** </span> VariedKBandit\n",
    "\n",
    "We now want to implement a Bandit with a range of distributions to sample from. Let's pick 5 interesting distributions corresponding to each available action:\n",
    "\n",
    "0. **Medical treatment**. reward: success of a treatment with success rate p=0.3. Success gives a reward of 10.\n",
    "1. **Slot machine**. reward: gain for a slot machine with playing cost of 1€, 10% probability to win 10€, and 1% probability to win 250€.\n",
    "2. **Goals in a Football match**. reward: [number of goals in a World Cup football match](https://en.wikipedia.org/wiki/Poisson_distribution#Examples_of_probability_for_Poisson_distributions).\n",
    "3. **Dice roll**. reward: roll of a dice. \n",
    "4. **Human height**. reward: [height](https://en.wikipedia.org/wiki/Average_human_height_by_country), in meters, of a random man taken from the joint populations of the Netherlands and Bolivia. Assume Gaussian distributions with standard deviations of [8cm](https://danielmiessler.com/blog/standard-deviations-explained); don't forget relative population sizes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariedKBandit(object):\n",
    "    \"\"\" Set of k=5 distributions as written above. \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.k = 5\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Resets each of the k bandits. \"\"\"\n",
    "        # TODO\n",
    "        raise NotImplementedError(\"VariedKBandit reset not implemented\")\n",
    "        \n",
    "    def best_action(self):\n",
    "        \"\"\" Resets each of the k bandits. \"\"\"\n",
    "        # TODO\n",
    "        raise NotImplementedError(\"VariedKBandit best_action not implemented\")\n",
    "        \n",
    "    def pull(self, action:int) -> float:\n",
    "        \"\"\"\n",
    "        Pulls the lever corresponding to the action.\n",
    "        In other words, samples a reward from the corresponding distribution.\n",
    "        Returns the reward.\n",
    "        Parameters\n",
    "        ----------\n",
    "        action: positive int < k\n",
    "            Lever to pull.\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            Reward for pulling this lever.\n",
    "        \"\"\"\n",
    "        # TODO: pull a lever to sample a reward given an action, return the reward.\n",
    "        raise NotImplementedError(\"VariedKBandit pull not implemented\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. <span style=\"color:red\"> **TODO** </span>  Plotting the results\n",
    "\n",
    "Try to change up the hyperparameters and understand what works and what doesn't, and why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETERS AND AGENTS ==================================================\n",
    "config = {\n",
    "    'k': 5,\n",
    "    'lr': 0.1,\n",
    "    'alpha': 0.25,\n",
    "    'eps': 0.1,\n",
    "    'c': 0.5,\n",
    "    'q0': 1\n",
    "}\n",
    "\n",
    "n_runs = 250\n",
    "max_steps = 2000 # time budget per run\n",
    "agents = [\n",
    "    Random_Agent(**config),\n",
    "    EpsGreedy_WeightedAverage(**config),\n",
    "    EpsGreedy_SampleAverage(**config),\n",
    "    OptimisticGreedy(**config),\n",
    "    Gradient_Bandit(**config),\n",
    "    UCB(**config)\n",
    "]\n",
    "\n",
    "kbandit = VariedKBandit(**config)\n",
    "\n",
    "perfs, best_actions = run_multiple_agents(agents=agents, n_runs=n_runs, kbandit=kbandit, max_steps=max_steps)\n",
    "labels = [agent.__class__.__name__ for agent in agents]\n",
    "suptitle = 'Agents Performances in Varied k-armed-Bandit'\n",
    "title = dict_string(config)\n",
    "\n",
    "action_plot (best_actions, suptitle, title, labels)\n",
    "perf_plot   (perfs,  suptitle, title, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **TODO** </span> Please explain your results in a paragraph in this cell.\n",
    "*Try to play with the hyperparameters (in `config`) to build intuition on how the algorithms work. You can decrease `n_runs` above so it runs a bit faster, but the results will be noisier. Please submit a complete unaltered run at the end.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}