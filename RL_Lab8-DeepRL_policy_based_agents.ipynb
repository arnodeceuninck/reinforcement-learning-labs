{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 8\n",
    "\n",
    "### What is A2C?\n",
    "\n",
    "Since the beginning of this course, we’ve studied two different reinforcement learning methods:\n",
    "\n",
    "* **Value based methods** (Q-learning, Deep Q-learning): where we learn a value function that will map each state action pair to a value.\n",
    "* **Policy based methods** (REINFORCE with Policy Gradients): where we directly optimize the policy without using a value function.\n",
    "\n",
    "But both of these methods have big drawbacks. That’s why, today, we’ll study a new type of Reinforcement Learning method which we can call a “hybrid method”: Actor Critic. We’ll using two neural networks:\n",
    "\n",
    "* **Actor**: Controls how our agent behaves (policy-based)\n",
    "* **Critic**:  Measures how good the state is (value-based)\n",
    "\n",
    "The Actor Critic model is a better score function. Instead of waiting until the end of the episode as we do in Monte Carlo REINFORCE, we make an update at each step (TD Learning).\n",
    "\n",
    "\n",
    "At the beginning, you don’t know how to play, so you try some action randomly. The Critic observes your action and provides feedback.\n",
    "Learning from this feedback, you’ll update your policy and be better at playing that game.\n",
    "\n",
    "On the other hand, your friend (Critic) will also update their own way to provide feedback so it can be better next time.\n",
    "As we can see, the idea of Actor Critic is to have two function approximator, the policy (actor) and the value function (critic). We estimate both with neural networks.\n",
    "\n",
    "Because we have two models (Actor and Critic) that must be trained, it means that we have two set of weights that must be optimized separately.\n",
    "\n",
    "#### Note:-\n",
    "Please *read* and *run* the notebook chronologically, and fill in the **TODO**s as you encounter them.\n",
    "* <span style=\"color:blue\"> Blue **TODOs** </span> means you have to implement the TODOs in the code.\n",
    "* <span style=\"color:red\"> Red **TODOs** </span> means you have to submit an explanation (of graph/results).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "train_env.seed(SEED);\n",
    "eval_env.seed(int(SEED/2))\n",
    "np.random.seed(SEED);\n",
    "torch.manual_seed(SEED);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> **TODO** </span> Write a network architecture that will be used for both actor and critic (general output). We want a hidden layer of size hidden_dim, which will be 256, and a dropout layer after the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.3):\n",
    "        \"\"\"\n",
    "        This is a basic Artificial Neural Network that can be used for both the Actor and the Critic.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "        \n",
    "        hidden_dim: int\n",
    "        \n",
    "        output_dim: int\n",
    "        \n",
    "        dropout: float\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = # TODO: Declare fully connected \n",
    "        self.fc_2 = # TODO: Declare fully connected\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Prediction of the Neural Network given an input x (in RL, x is a state).\n",
    "        The Network uses a dropout layer (to help generalize), and the ReLU activation function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: tensor\n",
    "            input, i.e. state\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x: tensor\n",
    "            the network's prediction (output) for this input.\n",
    "        \"\"\"\n",
    "        # TODO: write forward pass for NN\n",
    "        # Hint: it should have layers, dropout, relu.\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> **TODO** </span> Actor-Critic agent: contains both networks,  and outputs a value function and a policy (through a softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor, critic):\n",
    "        \"\"\"\n",
    "        This is a joint model, with two ANNs within.\n",
    "        Parameters\n",
    "        ----------\n",
    "        actor: BaseModel instance\n",
    "        \n",
    "        critic: BaseModel instance\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\" \n",
    "        The output of the ActorCritic model is the concatenation of the actor and critic's outputs.\n",
    "        Since the actor is a policy, we convert the output into probabilities using a softmax function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: tensor\n",
    "            model input, i.e. state the agent is in\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action_pred, value_pred: tensor, tensor\n",
    "            the network's prediction (output) for this input.\n",
    "        \"\"\"\n",
    "\n",
    "        action_pred = self.actor(state)\n",
    "        value_pred = self.critic(state)\n",
    "        \n",
    "        # TODO: convert actions to probabilities using softmax\n",
    "        action_pred = \n",
    "        \n",
    "        return action_pred, value_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Reinitialise agent again if you change any hyperparameter. Don't retrain same agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> **TODO** </span> Define the output dimensions of the actor and critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = train_env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# TODO: What should be the output dimention for actor and critic.\n",
    "# HINT: actor controls policy and critic outputs only values\n",
    "OUTPUT_DIM_ACTOR = \n",
    "OUTPUT_DIM_CRITIC = \n",
    "actor = BaseModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM_ACTOR)\n",
    "critic = BaseModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM_CRITIC)\n",
    "\n",
    "agent = ActorCritic(actor, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\" Initializes the ANNs weights with a relevant distribution. \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.02\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> **TODO** </span> Extract the action probability and state value from the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, optimizer, discount_factor):\n",
    "    \"\"\"\n",
    "    Performs a single training step over an episode.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent: ActorCritic\n",
    "    \n",
    "    optimizer: PyTorch optimizer\n",
    "    \n",
    "    discount_factor: float\n",
    "        discount gamma\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    policy_loss: float \n",
    "        loss of the policy (actor)\n",
    "    \n",
    "    value_loss: float\n",
    "        loss of the value function approximator (critic)\n",
    "    \n",
    "    episode_reward: float\n",
    "        reward for this episode\n",
    "    \"\"\"\n",
    "    agent.train()\n",
    "    \n",
    "    log_prob_actions = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # TODO: get action\n",
    "        action_prob = \n",
    "        # TODO: get value\n",
    "        value_pred = \n",
    "         \n",
    "        dist = distributions.Categorical(action_prob)\n",
    "\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        \n",
    "        if env_id == 'Pendulum-v0':\n",
    "            state, reward, done, _ = env.step(action)\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        values.append(value_pred)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_reward += reward\n",
    "    \n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    values = torch.cat(values).squeeze(-1)\n",
    "    \n",
    "    returns = calculate_returns(rewards, values, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy(advantages, log_prob_actions, returns, values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return policy_loss.item(), value_loss.item(), episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> **TODO** </span> Compute the return estimates with 3-step bootstrapping return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, values, discount_factor, normalize = True):\n",
    "    \"\"\"\n",
    "    Function to calculate rewards in time step order and normalize them.\n",
    "    Normalizing stabilizes the results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards: list of floats\n",
    "    \n",
    "    discount_factor: float\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    returns: tensor\n",
    "        tensor of returns G_t in time-step order\n",
    "    \"\"\"\n",
    "    # TODO: calculate future rewards\n",
    "\n",
    "    for i in range(<something>):\n",
    "        # TODO: Calculate n step return :\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> **TODO** </span> Calculate the advantages using hte TD error, and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values, normalize = True):\n",
    "    \"\"\"\n",
    "    Computes the advantage for all actions. \n",
    "    Reminder: the Advantage function for an action a is A(s,a) = Q(s,a) - V(s)\n",
    "    Normalizing stabilizes the results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns: tensor\n",
    "        Returns G_t during an episode\n",
    "    \n",
    "    values: tensor\n",
    "        Value estimates V(s_t)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    advantages: tensor\n",
    "    \"\"\"\n",
    "    # TODO: calculate advantage\n",
    "    advantages = \n",
    "    \n",
    "    # TODO: write code to normalize the values\n",
    "  \n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> **TODO** </span> Calculate the policy loss, using advantages and log_prob_actions; calculate the value loss with a smooth L1 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(advantages, log_prob_actions, returns, values):\n",
    "    \"\"\"\n",
    "    Function to update your policy based on your actor and critic loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    advantages: tensor\n",
    "    \n",
    "    log_prob_actions: tensor\n",
    "    \n",
    "    returns: tensor\n",
    "    \n",
    "    values: tensor\n",
    "    \n",
    "    optimizer: adam instance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    advantages = advantages.detach()\n",
    "    returns = returns.detach()\n",
    "    \n",
    "    # TODO: calculate policy loss based on advantages and log_prob_actions.\n",
    "    policy_loss = \n",
    "    \n",
    "    # TODO: calculate value loss based on Mean Absolute Error\n",
    "    value_loss = \n",
    "        \n",
    "    \n",
    "    return policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, vis=False):\n",
    "    \"\"\"\n",
    "    Function to evaluate your agent's performance.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    \n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            action_prob, _ = agent(state)\n",
    "                \n",
    "        action = torch.argmax(action_prob, dim = -1)\n",
    "                \n",
    "        if env_id == 'Pendulum-v0':\n",
    "            state, reward, done, _ = env.step(action)\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, train_rewards, policy_loss, value_loss):\n",
    "    \"\"\"\n",
    "    Plots the running reward and losses.\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame_idx: int\n",
    "        frame id\n",
    "    rewards: int\n",
    "        accumulated reward\n",
    "    losses: int\n",
    "        loss\n",
    "    \"\"\"\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('episode %s. reward: %s' % (frame_idx, np.mean(train_rewards[-10:])))\n",
    "    plt.plot(train_rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('policy loss')\n",
    "    plt.plot(policy_loss)\n",
    "    plt.subplot(133)\n",
    "    plt.title('value loss')\n",
    "    plt.plot(value_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 5000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 195\n",
    "\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "train_rewards = []\n",
    "test_rewards = []\n",
    "\n",
    "# init agent\n",
    "actor = BaseModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM_ACTOR)\n",
    "critic = BaseModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM_CRITIC)\n",
    "agent = ActorCritic(actor, critic)\n",
    "agent.apply(init_weights)\n",
    "optimizer = optim.Adam(agent.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "for episode in range(1, MAX_EPISODES+1):\n",
    "    \n",
    "    policy_loss, value_loss, train_reward = train(train_env, agent, optimizer, DISCOUNT_FACTOR)\n",
    "    test_reward = evaluate(eval_env, agent, False)\n",
    "    \n",
    "    train_rewards.append(train_reward)\n",
    "    test_rewards.append(test_reward)\n",
    "    policy_losses.append(policy_loss)\n",
    "    value_losses.append(value_loss)\n",
    "    \n",
    "    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "    \n",
    "    plot(episode, train_rewards, policy_losses, value_losses)\n",
    "\n",
    "    if mean_test_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluate():\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(test_rewards, label='Test Reward')\n",
    "    plt.plot(train_rewards, label='Train Reward')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Reward', fontsize=20)\n",
    "    plt.hlines(REWARD_THRESHOLD, 0, len(test_rewards), color='r')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL algorithms have many moving parts that are hard to debug, and they require substantial effort in tuning in order to get good results.\n",
    "PPO strikes as a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small.\n",
    "The central idea for Proximal Policy Optimization is to avoid having too large policy update\n",
    "\n",
    "PPO uses an clipping bound to control the change of the policy at each iteration. It uses an objective function not typically found in other algorithms.\n",
    "\n",
    "Here we implement PPO agent in A2C style. Which means it follows same actor critic implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinitialise your agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCritic(actor, critic)\n",
    "agent.apply(init_weights)\n",
    "optimizer = optim.Adam(agent.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting in details.\n",
    "So what makes PPO different from A2C?\n",
    "\n",
    "<span style=\"color:red\"> **TODO** </span> Explain theoretical difference between PPO and A2C in a paragraph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> **TODO** </span> Reuse A2C methods to implement custom PPO loss. Fill in all TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_ppo(agent, states, actions, log_prob_actions, advantages, returns, optimizer, ppo_steps, ppo_clip):\n",
    "    \n",
    "    total_policy_loss = 0 \n",
    "    total_value_loss = 0\n",
    "    \n",
    "    advantages = advantages.detach()\n",
    "    log_prob_actions = log_prob_actions.detach()\n",
    "    actions = actions.detach()\n",
    "    \n",
    "    for _ in range(ppo_steps):\n",
    "                \n",
    "        #get new log prob of actions for all input states\n",
    "        action_prob, value_pred = agent(states)\n",
    "        value_pred = value_pred.squeeze(-1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        \n",
    "        #new log prob using old actions\n",
    "        new_log_prob_actions = dist.log_prob(actions)\n",
    "        \n",
    "        # TODO calculate policy ratio\n",
    "        policy_ratio = \n",
    "        \n",
    "        # TODO calculate policy_loss_1, part of actor's loss          \n",
    "        policy_loss_1 = \n",
    "        \n",
    "        # TODO Calculate clipped part of actor's loss. Hint: check torch clamp.\n",
    "        policy_loss_2 = \n",
    "        \n",
    "        \n",
    "        policy_loss = - torch.min(policy_loss_1, policy_loss_2).sum()\n",
    "        \n",
    "        # TODO calculate value_loss\n",
    "        value_loss = \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        policy_loss.backward()\n",
    "        value_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_policy_loss += policy_loss.item()\n",
    "        total_value_loss += value_loss.item()\n",
    "    \n",
    "    return total_policy_loss / ppo_steps, total_value_loss / ppo_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, agent, optimizer, discount_factor, ppo_steps, ppo_clip):\n",
    "        \n",
    "    agent.train()\n",
    "        \n",
    "    states = []\n",
    "    actions = []\n",
    "    log_prob_actions = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        #append state here, not after we get the next state from env.step()\n",
    "        states.append(state)\n",
    "        \n",
    "        action_prob, value_pred = agent(state)\n",
    "                \n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        \n",
    "        if env_id == 'Pendulum-v0':\n",
    "            state, reward, done, _ = env.step(action)\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        actions.append(action)\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        values.append(value_pred)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        episode_reward += reward\n",
    "    \n",
    "    states = torch.cat(states)\n",
    "    actions = torch.cat(actions)    \n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    values = torch.cat(values).squeeze(-1)\n",
    "    \n",
    "    returns = calculate_returns(rewards, values, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy_ppo(agent, states, actions, log_prob_actions, advantages, returns, optimizer, ppo_steps, ppo_clip)\n",
    "\n",
    "    return policy_loss, value_loss, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 5000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 195\n",
    "PPO_STEPS = 5\n",
    "PPO_CLIP = 0.2\n",
    "\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "train_rewards = []\n",
    "test_rewards = []\n",
    "\n",
    "# init agent\n",
    "actor = BaseModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM_ACTOR)\n",
    "critic = BaseModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM_CRITIC)\n",
    "agent = ActorCritic(actor, critic)\n",
    "agent.apply(init_weights)\n",
    "optimizer = optim.Adam(agent.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "for episode in range(1, MAX_EPISODES+1):\n",
    "    \n",
    "    policy_loss, value_loss, train_reward = train_ppo(train_env, agent, optimizer, DISCOUNT_FACTOR, PPO_STEPS, PPO_CLIP)\n",
    "    \n",
    "    test_reward = evaluate(eval_env, agent)\n",
    "    \n",
    "    train_rewards.append(train_reward)\n",
    "    test_rewards.append(test_reward)\n",
    "    policy_losses.append(policy_loss)\n",
    "    value_losses.append(value_loss)\n",
    "\n",
    "    \n",
    "    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "    \n",
    "    plot(episode, train_rewards, policy_losses, value_losses)\n",
    "    \n",
    "    if mean_test_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}