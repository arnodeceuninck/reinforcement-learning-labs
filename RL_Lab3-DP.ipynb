{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name**:\n",
    "    \n",
    "**Student ID**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Dynamic Programming\n",
    "\n",
    "### Instructions: **TODO** tags\n",
    "The first part of this lab is simply help on theory, in case you had questions. Since you will have to write formulas in $\\LaTeX$, it also contains help on this. In the second part you will have to implement Dynamic Programming methods to solve MDPs. The way it will work is very similar to the previous labs.\n",
    "\n",
    "As usual, please *read* and *run* the notebook chronologically, and fill in the **TODO**s as you encounter them.\n",
    "* <span style=\"color:blue\"> Blue **TODOs** </span> means you have to implement the TODOs in the code.\n",
    "* <span style=\"color:red\"> Red **TODOs** </span> means you have to submit an explanation (of graph/results/theory).\n",
    "\n",
    "\n",
    "At each section, <span style=\"color:green\"> (xx points) </span> indicates the number of points of the entire section (labs are graded out of 10).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> 1. Help on Theory </span>\n",
    "This part is only meant to be read and is **optional**. It will hopefully give some understanding of the expectations that come up all the time in Dynamic Programming. \n",
    "\n",
    "### <span style=\"color:green\"> 1.1. Trajectories and Expectations</span>\n",
    "*Optional read. This paragraph aims at clarifying why the expectations show up everywhere in the formulas, and what they mean intuitively. If you're interested, give it a read!*\n",
    "\n",
    "At the heart of Reinforcement Learning is the idea of interactions between Agent and Environment. These interactions give rise to a **trajectory** of 3 alternating elements: state, action, reward; and the next state to repeat the process. A trajectory is therefore \n",
    "$$ \\tau = \\left( S_0, A_0, R_1, S_1, A_1, R_2, S_2... \\right) $$\n",
    "It is crucial to understand that a trajectory is generated by two distributions - from $S_t$ to $A_t$ sampling from the Agent's policy $\\pi$, and from $S_t,A_t$ to $R_{t+1},S_{t+1}$ sampling from the Environment's dynamics $p$.\n",
    "\n",
    "The intuitive goal of RL is to maximize the sum of rewards $\\sum_t R_t$ obtained during the trajectory, this is the *return* $G_t$ (starting from some step $t$ and summing over the future steps $t^\\prime$). This process is **very chaotic**. We are alternatively and iteratively sampling from two different arbitrary distributions. Therefore, we rather want to maximize the *expected* return. The intuition is that \"you can not always get the best results (imagine the environment is \"playing poker\"), so we focus on doing the best possible on average\". The discount does not change the intuition.\n",
    "\n",
    "The expectations we take are therefore *over the trajectory*, or in other words, *over the alternating samplings from $\\pi$ and $p$*. A notation for that is $$\\mathbb{E}_{\\pi,p}\\left[ \\cdot \\right]$$\n",
    "Since we have no control over the dynamics $p$, we usually simplify this expression to $\\mathbb{E}_{\\pi}\\left[ \\cdot \\right]$ to only what we will be changing and optimizing. However, it is important to realize that this expectation also depends on the dynamics $p$.\n",
    "\n",
    "Hopefully this demystifies a bit the formulas, which can quickly look scary.\n",
    "\n",
    "### <span style=\"color:green\"> 1.2. One-step Expectations in Bellman Backups </span> \n",
    "*Optional read. This paragraph now aims at clarifying the one-step-ahead expectations that come up in most of the Bellman equations*.\n",
    "\n",
    "You have seen in the course that the core idea of all Bellman equations is the observation that $G_t = R_{t+1} + \\gamma G_{t+1}$. The discounted return at this step is a simple function of the reward you will get, and the next return. However, how do we use this fact practically?\n",
    "\n",
    "The intuition is that you want to look one step ahead in the trajectory. As we know, during one step, there is one sampling from $\\pi$ and one sampling from $p$. So what we want to do is break the expectation down into 1/ the next step, 2/ the rest. \n",
    "\n",
    "Remember that the general way of taking an expectation for the finite case is $\\sum_i p_i x_i$ - in other words, scale each possible output $x_i$ with its probability to happen $p_i$.\n",
    "\n",
    "How do we take an expectation over a single step in the MDP? Well, imagine that you're sitting on a state $s$. From there, you have a choice of $\\left| \\mathcal{A} \\right|\\doteq k$ actions. Which action you take will depend on your policy $\\pi$, and that will get you sitting on the corresponding state-action node. So if you wanted to take an expectation of some variable $x$ over actions, you would get $\\sum_a \\pi\\left(a \\mid s\\right)x_a$.\n",
    "\n",
    "Now this was just getting us to the state-action node. Now that we are sitting on the state-action node, there's a bunch of things that can happen - all the state-reward pairs that the dynamics can lead us to. Therefore if you wanted to take an expectation of some value $x$ over state-reward pairs from this state-action node, you would get $\\sum_{s^ \\prime, r} p\\left(s^\\prime,r \\mid s,a\\right)x_{s^\\prime,r}$. The convenient joint-sum notation means \"all possible state-reward pairs\". \n",
    "\n",
    "Now you're back at a state node, and you can repeat the process if you want. You will have taken an expectation over $x$ of one full step in the MDP: from a state $s$, $\\sum_a \\pi\\left(a \\mid s\\right) \\sum_{s^ \\prime, r} p\\left(s^\\prime,r \\mid s,a\\right)x_{a,s^\\prime,r}$\n",
    "\n",
    "Most of the time, what we will be interested about is the expected return, so $x$ will be some variation of the return (actual return, expected return, value function...).\n",
    "\n",
    "### <span style=\"color:green\"> 1.3 Help on $\\LaTeX$  </span> \n",
    "[Click on this link](https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd) to see a whole bunch of usages of inline $\\LaTeX$ math.\n",
    "\n",
    "Below are examples of things you can use in your formulas, if needed. Enter the cell to see and copy-paste the elements.\n",
    "- Greek letters (capitals or not): $\\pi, \\Pi$; $\\rho$; $\\theta, \\Theta$; $\\delta, \\Delta$\n",
    "- Sets: $\\mathbb{N,R}$ $\\mathcal{A,S}$\n",
    "- Underscore and exponent: $x_i ^j$; $e^{i \\pi} +1 = 0$\n",
    "- Prime: $s^\\prime$, $\\pi^\\prime$\n",
    "- Sum and product: $\\sum_{a \\in \\mathcal{A}}$ $\\prod_{t=0}^T u_t v_t$\n",
    "- Max, argmax and underscoring: $\\max_a$, $\\underset{a}{\\arg \\max}$\n",
    "- Parenthesis and brackets of variable size: $f\\left( \\sum_i \\left[ r_i + x_i\\right]\\right)$\n",
    "- Conditional distribution: $f\\left( x \\mid y \\right)$\n",
    "- Display mode and random helpful things:\n",
    "$$\n",
    "\\text{we know that } \\lim_{x \\to c} f(x) = L  \\iff  (\\forall \\varepsilon > 0,\\,\\exists \\ \\delta > 0,\\,\\forall x \\in D,\\,0 < |x - c| < \\delta \\ \\Rightarrow \\ |f(x) - L| < \\varepsilon)\n",
    "$$\n",
    "- Alignement:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(x+y \\right)^2 &= x^2 + xy + yx + y^2\n",
    "                    &= x^2 + 2xy + y^2\n",
    "\\end{align}\n",
    "$$\n",
    "- Arrays:\n",
    "$$\n",
    "sign(x) = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        1 & \\mbox{if } \\ x \\in \\mathbf{N}^* \\\\\n",
    "        0 & \\mbox{if } \\ x = 0 \\\\\n",
    "        -1 & \\mbox{else.}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "- Logical reasoning: \"therefore\": $\\therefore$    \"is defined as\": $\\doteq$    \"if and only if\": $\\iff$    \"implies\": $\\implies$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <span style=\"color:green\"> (7 points) </span>  Dynamic Programming: Policy Iteration\n",
    "Now we will implement the Value Iteration and Policy Iteration algorithms from the course. \n",
    "First, let's import the required libraries and the environment you implemented last time. You don't have anything to do here, this was the subject of the last lab. The MDP might be implemented slightly differently from what you did, it's fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the environment you implemented last lab! Nothing to do here.\n",
    "class CourseEnv(gym.Env):\n",
    "    \"\"\" Gridworld environment from the Course. A 4x3 grid with 2 states in the upper right corner \n",
    "    leading to the terminal state.\n",
    "        \"\"\"\n",
    "    def __init__(self):\n",
    "        self.height = 3\n",
    "        self.width = 4\n",
    "        self.action_space = spaces.Discrete(5) # maximum amount of actions possible in a state\n",
    "\n",
    "        self.observation_space = spaces.Tuple(( # observations come in (x,y) tuples with x:height, y:width.\n",
    "                spaces.Discrete(self.height),\n",
    "                spaces.Discrete(self.width)\n",
    "                ))\n",
    "        self.moves = { # converts actions to 2D moves\n",
    "                'north': (-1, 0),\n",
    "                'east':  (0, 1),\n",
    "                'south': (1, 0),\n",
    "                'west' : (0, -1),\n",
    "                }\n",
    "        self.moves_list = [ # enables shortcuts in applying noise\n",
    "                'north',\n",
    "                'east',\n",
    "                'south',\n",
    "                'west',\n",
    "                ]\n",
    "        self.noise = .2\n",
    "        self.start = (2,0)\n",
    "        self.near_terminals = ((0,3), (1,3)) # arbitrary terminal states\n",
    "        self.obstacles = [(1,1)]\n",
    "        self.living_reward = -0.1\n",
    "        # begin in start state\n",
    "        self.reset()\n",
    "\n",
    "    def _move(self, action, state=None):\n",
    "        \"\"\" Moves the agent according to the required action, taking hitboxes into account. \"\"\"\n",
    "        dx, dy = self.moves[action]\n",
    "        if state is None:\n",
    "            state = self.S\n",
    "            \n",
    "        state = state[0] + dx, state[1] + dy\n",
    "\n",
    "        if state in self.obstacles: # cancel movement\n",
    "            state = state[0] - dx, state[1] - dy            \n",
    "        \n",
    "        # Finally, setting the agent back into the grid if fallen out\n",
    "        state = (max(0, state[0]), max(0, state[1]))\n",
    "        state = (min(state[0], self.height - 1),\n",
    "                 min(state[1], self.width - 1))\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" Moves the agent in the action direction.\n",
    "            \"\"\"\n",
    "        if self.S is 'TERMINAL':\n",
    "            raise ValueError(\"Trying to step from TERMINAL state.\")\n",
    "\n",
    "        if self.S in self.near_terminals:\n",
    "            assert action == 'exit', \"Non-exit action in state {}\".format(self.S)\n",
    "            if self.S == self.near_terminals[0]: # rewarding state\n",
    "                return 'TERMINAL', +1, True, {}\n",
    "            if self.S == self.near_terminals[1]: # punishing state\n",
    "                return 'TERMINAL', -1, True, {}\n",
    "            \n",
    "        assert action in self.moves_list, \"invalid action {} in state {}\".format(action, self.S)\n",
    "        # Otherwise, moving according to action.\n",
    "        # First, maybe apply some noise:\n",
    "        if np.random.rand() < self.noise: # Apply noise!\n",
    "            action = self.moves_list[(self.moves_list.index(action)+np.random.choice((-1,1))) % 4]\n",
    "        \n",
    "        print(\"action executed: {}\".format(action))\n",
    "        self.S = self._move(action)\n",
    "        \n",
    "        # Returns; anything brings a reward of living\n",
    "        return self.S, self.living_reward, self.S is 'TERMINAL', {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.S = self.start\n",
    "        return self.S\n",
    "    \n",
    "    def available_actions(self, state=None):\n",
    "        \"\"\"\n",
    "        List of available actions in the provided state\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: tuple (position), string ('TERMINAL') or None\n",
    "            state from which to provide all actions. If None, use the current environment state.\n",
    "        Returns\n",
    "        -------\n",
    "        ret : list\n",
    "            List of all actions available in the provided state.\n",
    "        \"\"\"\n",
    "        if state is None:\n",
    "            state = self.S\n",
    "        if state is 'TERMINAL':\n",
    "            return []\n",
    "        if state in self.near_terminals:\n",
    "            return ['exit']\n",
    "        \n",
    "        return self.moves_list\n",
    "    \n",
    "    def p(self, state, action):\n",
    "        \"\"\"\n",
    "        Dynamics function p of the MDP in this state and action.\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: tuple (position) or string ('TERMINAL')\n",
    "            state from which to provide all actions. If the terminal state is provided, raises an error, \n",
    "            as there are no dynamics from the terminal state. \n",
    "        action: string \n",
    "            in list in ['north', 'east', 'south', 'west', 'exit'] with proper state\n",
    "        Returns\n",
    "        -------\n",
    "        ret : dict\n",
    "            dictionary of (next_state, reward) pairs with: corresponding probabilities\n",
    "        \"\"\"\n",
    "        # Terminal state: return error\n",
    "        assert state is not 'TERMINAL', \"asking for dynamics from terminal state\"\n",
    "        # Near terminal state\n",
    "        if state in self.near_terminals:\n",
    "            assert action == 'exit', \"Non exit action ({}) in near terminal state\".format(action)\n",
    "            if state == self.near_terminals[0]:\n",
    "                return {('TERMINAL', +1): 1.}\n",
    "            if state == self.near_terminals[1]:\n",
    "                return {('TERMINAL', -1): 1.}\n",
    "        # Other states: 3 possibilities: normal development of the action doing its job, or noise hindering        \n",
    "        action_n1 = self.moves_list[(self.moves_list.index(action)-1) % 4] # noise-impacted action 1\n",
    "        action_n2 = self.moves_list[(self.moves_list.index(action)+1) % 4] # noise-impacted action 2\n",
    "        d = {}\n",
    "        # The main problem is to make sure you're not counting the same state-reward pair twice\n",
    "        # instead of summing the probabilities\n",
    "        d[(self._move(action, state), self.living_reward)] = 1-self.noise\n",
    "        sr2 = (self._move(action_n1, state), self.living_reward)\n",
    "        if sr2 in d.keys():\n",
    "            d[sr2] += self.noise/2\n",
    "        else:\n",
    "            d[sr2] = self.noise/2\n",
    "        \n",
    "        sr3 = (self._move(action_n2, state), self.living_reward)\n",
    "        if sr3 in d.keys():\n",
    "            d[sr3] += self.noise/2\n",
    "        else:\n",
    "            d[sr3] = self.noise/2\n",
    "        return d\n",
    "    \n",
    "    def is_terminal(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.S\n",
    "        return state is 'TERMINAL'\n",
    "    \n",
    "    def states(self):\n",
    "        states = [(x,y) for x in range(self.height) for y in range(self.width) if (x,y) not in self.obstacles]\n",
    "        states += ['TERMINAL']\n",
    "        return states\n",
    "        \n",
    "    def render(self):\n",
    "        s = np.zeros((self.height, self.width), dtype=int).astype(str)\n",
    "        s[self.start] = 'S'\n",
    "        s[self.obstacles[0]] = 'X'\n",
    "        s[self.near_terminals[0]] = '+'\n",
    "        s[self.near_terminals[1]] = '-'\n",
    "        s[self.S] = '.'\n",
    "        \n",
    "        print(self.S)\n",
    "        print(s)\n",
    "        print(\"Available actions: {}\".format(self.available_actions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions will be useful to plot your Value function and visualize what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing to do here. \n",
    "def to_np(value_dict):\n",
    "    \"\"\" Turns a value dictionary of our CourseEnv into a numpy array.\"\"\"\n",
    "    grid = np.zeros((3,4))\n",
    "    for k,v in value_dict.items():\n",
    "        if type(k) is not str:\n",
    "            grid[k] = v\n",
    "            \n",
    "    return grid\n",
    "\n",
    "def plot_value_policy(agent):\n",
    "    \"\"\" Visualizes a policy and value function given an agent with V and policy.\"\"\"\n",
    "    cmap = clr.LinearSegmentedColormap.from_list('mycmap', ['#FF0000','#000000', '#008000'])\n",
    "    grid = to_np(agent.V)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(grid, cmap=cmap)\n",
    "    for i in range(agent.mdp.height):\n",
    "        for j in range(agent.mdp.width):\n",
    "            s = (i,j)\n",
    "            if s not in agent.mdp.obstacles:\n",
    "                text = ax.text(j, i, agent.policy(s) + \"\\n{:04.2f}\".format(agent.V[s]),\n",
    "                               ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Value and Policy visualization\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {
    "backup_v_pi.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. <span style=\"color:red\"> **TODO** </span> Policy Improvement Theorem\n",
    "As a reminder, the Policy Iteration and Value Iteration algorithms are applications of the Policy Improvement Theorem. The theorem states that to get an optimal policy, you can simply alternate **Policy Evaluation**, i.e. estimating the vaue $v_\\pi$ of the current policy $\\pi$, and **Value Improvement**, i.e. improving on $\\pi$ by using a greedy policy over $v_\\pi$\n",
    "\n",
    "- **Policy Evaluation** is using the *Bellman backup operation*. It is simply an iterative version of the Bellman equation for the value function (BEVF): since we know it is a contraction converging to $v_\\pi$, we keep applying it. The BEVF might seem complicated, but can be visualized simply using the *backup diagram* of the value function: <img src=\"attachment:backup_v_pi.png\" width=\"200\"> In this graph, multiple brances branching out from a node (either a state or state-action node) means doing an *expectation* over the relevant distribution (policy from a state, dynamics from a state-action pair). If you want to visualize how to get from the backup diagram to the formula, [we did an animation](https://www.youtube.com/watch?v=1p7Zgy79cSo)!\n",
    "- **Policy Improvement** is creating a policy $\\pi^\\prime$ that improves on current policy $\\pi$. This can be done, according to the *Policy Improvement Theorem*, simply by acting greedily over the value function of $\\pi$, $v_\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** the following questions about Policy Iteration:\n",
    "- <span style=\"color:red\"> **TODO** </span>: Write in [$\\LaTeX$](https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd) the iterative formula for Policy Evaluation in a given state $s$.\n",
    "$$ \\text{your answer here} $$\n",
    "- <span style=\"color:red\"> **TODO** </span>: Write in [$\\LaTeX$](https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd) the formula for Policy Improvement in a given state s, as a function of the state-action value function $q_\\pi$.\n",
    "$$ \\text{your answer here} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Note on Agent Evaluation </span>\n",
    "Policy Iteration and Value Iteration can be tricky to compare, because they use different backups and convergence methods. In this lab, we will count the number of **sweeps** over the state space that the algorithm needed to do to converge. In other words, doing the following:\n",
    "```\n",
    "for s in mdp.states():\n",
    "    # some operation\n",
    "```\n",
    "is considered a single sweep over the state space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. <span style=\"color:blue\"> **TODO** </span> Policy Iteration\n",
    "Policy Iteration does Policy Evaluation until some arbitrary precision $\\delta$, followed by one step of policy improvement, iteratively. Unlike Value Iteration, it maintains an explicit policy. This policy is *deterministic*: it always picks the same action given the same state - in other words, the distribution is an [indicator](https://en.wikipedia.org/wiki/Indicator_function) of action $a$. Having a deterministic policy makes the Policy Evaluation steps shorter.\n",
    "\n",
    "- <span style=\"color:red\"> **TODO** </span>: **Adapt** the iterative formula for Policy Evaluation to a deterministic policy. **Why** is this step faster than for a non-deterministic policy?\n",
    "$$ \\text{your answer here} $$\n",
    "<br/> (your answer here)\n",
    "- <span style=\"color:red\"> **TODO** </span>: **When** does the Policy Iteration algorithm stop? <br/> (your answer here)\n",
    "\n",
    "- <span style=\"color:blue\"> **TODO** </span> **implement** below the Policy Iteration algorithm. Apply your Policy Evaluation steps until $\\left|\\left| v_{k+1} - v_k \\right|\\right|_\\infty < \\delta$. ([infinity norm reminder](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm))). **Please use** the dictionary `self.pi` for the policy and the dictionary `self.V` for the values, so that we can print them easily. **Please use** the `plot_value_policy(self)` method for plotting the current state of the algorithm *at the end of each Policy Evaluation*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration():\n",
    "    \"\"\" Dynamic Programming Agent that performs Policy Iteration.\"\"\"\n",
    "\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp # MDP we're trying to solve\n",
    "        self.gamma = 0.9\n",
    "        self.V = {} # values dictionary\n",
    "        self.pi = {} # policy dictionary\n",
    "        # TODO: initialize your value and policy dictionaries. The policy should be random.\n",
    "        raise NotImplementedError(\"Initialization of the values V and pi dictionaries\")\n",
    "        \n",
    "        # Plotting. You can use this method in the `run` method as well.\n",
    "        print(\"Initial Value and Policy:\")\n",
    "        plot_value_policy(self)\n",
    "\n",
    "    def policy(self,s):\n",
    "        return self.pi[s]\n",
    "\n",
    "    def run(self, delta=1e-5):\n",
    "        \"\"\"\n",
    "        Runs the Policy Iteration algorithm until convergence. \n",
    "        The Policy Evaluation steps are run until inf norm < delta.\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta: float\n",
    "            Precision required to exit Policy Evaluation, in terms of inf norm.\n",
    "        Returns\n",
    "        -------\n",
    "        sweeps : int\n",
    "            Number of performed sweeps over the state space\n",
    "        \"\"\"\n",
    "        sweeps = 0\n",
    "        # TODO: Implement the Policy Iteration algorithm.\n",
    "        raise NotImplementedError(\"run function of Policy Iteration\")\n",
    "        return sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize and Visualize\n",
    "mdp = CourseEnv()\n",
    "agent = PolicyIteration(mdp)\n",
    "print(\"Initial Value & Policy:\")\n",
    "plot_value_policy(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and Visualize\n",
    "sweeps = agent.run(delta=1e-3)\n",
    "print(\"Final Value & Policy:\")\n",
    "plot_value_policy(agent)\n",
    "print(\"Done in {} sweeps over the state space.\".format(sweeps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **TODO** </span> **Discuss** briefly your approach and the results you obtained.\n",
    "\n",
    "(your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <span style=\"color:green\"> (3 points) </span> Value Iteration \n",
    "### 3.1.  <span style=\"color:blue\"> **TODO** </span> Classical Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Policy Iteration performs Policy Evaluation until convergence, and one step of Policy Improvement, Value Iteration alternates a single step of each. Due to this, we can actually introduce the Policy Improvement step directly in the formula of Policy Evaluation.\n",
    "\n",
    "- <span style=\"color:red\"> **TODO** </span> : **Show** in [$\\LaTeX$](https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd) how to combine a step of Policy Improvement and Policy Evaluation to obtain the iterative formula for Value Iteration in a given state $s$.\n",
    "$$ \\text{your answer here} $$\n",
    "\n",
    "- **Explain** the differences in algorithm complexity and approach between Policy Iteration and Value Iteration. <br/> (your answer here) \n",
    "- <span style=\"color:blue\"> **TODO** </span> **implement** below the Value Iteration algorithm. Apply it until $\\left|\\left| v_{k+1} - v_k \\right|\\right|_\\infty < \\delta$. ([infinity norm reminder](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm))) Be careful, **this is not yet the Asynchronous version**! **Please use** the dictionary `self.V` for the values, so that we can print them easily. **Please use** the `plot_value_policy(self)` method for plotting the current state of the algorithm *every 10th sweep*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration():\n",
    "    \"\"\" Dynamic Programming Agent that performs ValueIteration.\"\"\"\n",
    "\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp # MDP we're trying to solve\n",
    "        self.gamma = 0.9\n",
    "        self.V = {} # values dictionary\n",
    "        # TODO: Initialize the values dictionary\n",
    "        raise NotImplementedError(\"Initialization of the values dictionary V\")\n",
    "\n",
    "    def policy(self, s):\n",
    "        \"\"\"\n",
    "        Policy of the Agent; in other words, returns the action chosen by ValueIteration in a given state.\n",
    "        Parameters\n",
    "        ----------\n",
    "        s: State (tuple or str)\n",
    "            State from which to pick an action.\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Chosen action within the action space.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the implicit policy of the Value Iteration algorithm.\n",
    "        raise NotImplementedError(\"policy function of the Value Iteration algorithm\")\n",
    "    \n",
    "    def run(self, delta=1e-5):\n",
    "        \"\"\"\n",
    "        Runs the Value Iteration algorithm until convergence. \n",
    "        The iterations are ran until inf norm < delta.\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta: float\n",
    "            Precision required to exit the algo, in terms of inf norm.\n",
    "        Returns\n",
    "        -------\n",
    "        sweeps : int\n",
    "            Number of performed sweeps over the state space\n",
    "        \"\"\"\n",
    "        sweeps = 0\n",
    "        # TODO: Implement the Value Iteration algorithm.\n",
    "        raise NotImplementedError(\"run function of the Value Iteration algorithm\")\n",
    "        return sweeps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Visualize\n",
    "mdp = CourseEnv()\n",
    "agent = ValueIteration(mdp)\n",
    "print(\"Initial Value & Policy:\")\n",
    "plot_value_policy(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and Visualize\n",
    "sweeps = agent.run(delta=1e-3)\n",
    "print(\"Final Value & Policy:\")\n",
    "plot_value_policy(agent)\n",
    "print(\"Done in {} sweeps over the state space.\".format(sweeps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **TODO** </span> **Discuss** briefly your approach and the results you obtained.\n",
    "\n",
    "(your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.<span style=\"color:blue\"> **TODO** </span> Asynchronous Value Iteration: *In-Place*\n",
    "The ValueIteration algorithm we just implemented uses our $v$ estimate from the *previous iteration*, $v_k$, to perform the backup. However, we might have more recent value, in the sweep that is currently occurring. Why not use the best value estimate? This is called **In Place** Value Iteration, and is one of the simplest ideas of **Asynchronous** Dynamic Programming, where the order of the states and estimates used for the backups can be chosen more smartly than the naive sweeps we have done so far.\n",
    "\n",
    "* <span style=\"color:blue\"> **TODO** </span> **Implement** the changes to Value Iteration discussed above to obtain an In-Place Value Iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InPlaceValueIteration(ValueIteration):\n",
    "    # TODO: Implement the changes to ValueIteration to make it In-Place.\n",
    "    raise NotImplementedError(\"TODO: Implement the changes to ValueIteration to make it In-Place.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Visualize\n",
    "mdp = CourseEnv()\n",
    "agent = InPlaceValueIteration(mdp)\n",
    "print(\"Initial Value & Policy:\")\n",
    "plot_value_policy(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and Visualize\n",
    "sweeps = agent.run(delta=1e-3)\n",
    "print(\"Final Value & Policy:\")\n",
    "plot_value_policy(agent)\n",
    "print(\"Done in {} sweeps over the state space.\".format(sweeps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **TODO** </span> **Explain** briefly your approach and the results you obtained.\n",
    "\n",
    "(your answer here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
