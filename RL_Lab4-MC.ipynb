{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name**:\n",
    "\n",
    "**Student ID**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Monte Carlo Methods\n",
    "\n",
    "### Instructions: **TODO** tags\n",
    "In this lab, you will implement Monte Carlo methods. In particular, there will be 3 main steps:\n",
    "1. **Policy Evaluation**: provided an arbitrary policy $\\pi$, compute its value function $v_\\pi$ using MC. You will be asked to compare your results to the exact solution you will find with Dynamic Programming.\n",
    "2. **On-Policy MC Control**: implement MC to find an optimal soft policy, using an $\\epsilon$-greedy policy.\n",
    "3. **Off-Policy MC Control**: same idea using an $\\epsilon$-greedy behavior policy and learning the optimal policy.\n",
    "\n",
    "As usual, please *read* and *run* the notebook chronologically, and fill in the **TODO**s as you encounter them.\n",
    "* <span style=\"color:blue\"> Blue **TODOs** </span> means you have to implement the TODOs in the code.\n",
    "* <span style=\"color:red\"> Red **TODOs** </span> means you have to submit an explanation (of graph/results/theory).\n",
    "\n",
    "At each section, <span style=\"color:green\"> (xx points) </span> indicates the number of points of the entire section (labs are graded out of 10).\n",
    "\n",
    "\n",
    "### Environment\n",
    "Below are the libraries we will use and the Environment.\n",
    "\n",
    "The Environment for this lab and the next lab on tabular RL is called the **Four Rooms** Environment. It simply contains 4 rooms of fixed size $5\\times5$ in our case- in which the agent can move around, as in a house. There is no noise in the dynamics, meaning that the 4 directional actions deterministically take you to the requested neighboring state. Unless stated otherwise, the agent starts from the upper left corner. It will always have to reach the terminal state in the lower right corner to get a sparse reward of $+1$; any other transition gives reward $0$. A visualization will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clr\n",
    "from matplotlib import cm\n",
    "import random\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourRoomsEnv(gym.Env):\n",
    "    \"\"\" Small Gridworld environment with 4 rooms.\n",
    "    Starting up left, goal in lower-right.\n",
    "    The main challenge is that the reward is sparse (1_goal)\n",
    "        \"\"\"\n",
    "    def __init__(self, explo_starts=False, max_steps=None):\n",
    "        self.roomsize = 5\n",
    "        self.height = 2*self.roomsize +1 # +1 is obstacle width\n",
    "        self.width = self.height\n",
    "        half = self.width // 2 # shortcut\n",
    "        quarter = half // 2 # shortcut\n",
    "        self.gamma = 0.9\n",
    "        self.t = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.explo_starts = explo_starts\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "                spaces.Discrete(self.height),\n",
    "                spaces.Discrete(self.width)\n",
    "                ))\n",
    "        self.shapes = ((11,11), 4)\n",
    "\n",
    "        self.moves = {\n",
    "                0: (-1, 0),  # up\n",
    "                1: (0, 1),   # right\n",
    "                2: (1, 0),   # down\n",
    "                3: (0, -1),  # left\n",
    "                }\n",
    "        self.moves_to_str = {\n",
    "                0: u'↑',\n",
    "                1: u'→',\n",
    "                2: u'↓',\n",
    "                3: u'←',\n",
    "                }\n",
    "        \n",
    "        self.terminal = (self.height-1,self.width-1) # terminal state\n",
    "        horizontal  = [(i, half) for i in range(self.width)]\n",
    "        vertical    = [(half, i) for i in range(self.height)]\n",
    "        self.obstacles = horizontal + vertical\n",
    "        # now opening the 4 passages\n",
    "        for state in [(quarter,half), (half,quarter), (half,self.height-quarter), (self.height-quarter,half)]:\n",
    "            self.obstacles.remove(state)\n",
    "        self.start = (0,0)\n",
    "        # begin in start state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.max_steps is not None:\n",
    "            self.t = 0\n",
    "        if self.explo_starts:\n",
    "            while True:\n",
    "                s = self.observation_space.sample()\n",
    "                if s not in self.obstacles + [self.terminal]:\n",
    "                    break\n",
    "            self.s = s\n",
    "            #print(\"Random Start at S={}\".format(s))\n",
    "                \n",
    "        else:\n",
    "            self.s = self.start\n",
    "        return self.s\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" Moves the agent in the action direction.\"\"\"\n",
    "        # Next, moving according to action\n",
    "        x, y = self.moves[action]\n",
    "        if (self.s[0]+x, self.s[1]+y) not in self.obstacles:\n",
    "            # move is allowed\n",
    "            self.s = self.s[0] + x, self.s[1] + y\n",
    "\n",
    "            # Finally, setting the agent back into the grid if fallen out\n",
    "            self.s = (max(0, self.s[0]), max(0, self.s[1]))\n",
    "            self.s = (min(self.s[0], self.height - 1),\n",
    "                      min(self.s[1], self.width - 1))\n",
    "\n",
    "        self.t += 1\n",
    "        done_goal = (self.s == self.terminal)\n",
    "        done = done_goal\n",
    "        if self.max_steps is not None and self.t == self.max_steps:\n",
    "            done = True\n",
    "            self.t = 0\n",
    "        return self.s, int(done_goal), done, {}\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        return state == self.terminal\n",
    "    \n",
    "    def p(self, state, action):\n",
    "        assert not self.is_terminal(state)\n",
    "        self.s = state\n",
    "        next_state, reward, done, info = self.step(action)\n",
    "        return {(next_state, reward): 1}\n",
    "    \n",
    "    def states(self):\n",
    "        ss = []\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if (i,j) not in self.obstacles:\n",
    "                    ss.append((i,j))\n",
    "        return ss \n",
    "    \n",
    "    def plot_values_policy(self, values, policy, plot_all_policy=False, cbar=True):\n",
    "        \"\"\" Visualizes a policy and value function given an agent with V and policy.\"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        values[tuple(zip(*self.obstacles))] = None # -1\n",
    "        cmap = copy(cm.get_cmap(\"RdYlGn\"))\n",
    "        # modify colormap\n",
    "        l = np.array([cmap(i) for i in range(cmap.N)]).T\n",
    "        l[0:2] = np.clip(l[0:2], 0,0.9)\n",
    "        l[2] = np.clip(l[2], 0,0.25)\n",
    "        l = l.T\n",
    "        my_cmap = matplotlib.colors.ListedColormap(l, name = 'nani')\n",
    "        my_cmap.set_bad('black')\n",
    "\n",
    "        im = ax.imshow(values, cmap=my_cmap)\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                s = (i,j)\n",
    "                if s not in self.obstacles:\n",
    "                    t = self.moves_to_str[policy[s]] if not self.is_terminal(s) \\\n",
    "                                                        and ((values[s] != 0) or (plot_all_policy)) else \"\"\n",
    "                    text = ax.text(j, i, t,\n",
    "                                   ha=\"center\", va=\"center\", color=\"w\", fontsize=15)\n",
    "\n",
    "        ax.set_title(\"Value function and Policy visualization\")\n",
    "        fig.tight_layout()\n",
    "        if cbar:\n",
    "            plt.colorbar(im)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <span style=\"color:green\"> (5 points) </span>  Policy Evaluation\n",
    "\n",
    "### 1.1 Defining and visualizing the policy\n",
    "Let's define and visualize an arbitrary *deterministic* policy $\\pi$. We're picking a deterministic policy for ease of visualization and understanding of the learning process. \n",
    "\n",
    "Since we know we only need to go south and east to solve the task, let's use a policy that only picks between these two. We're plotting in green the terminal state we want to reach; later, the colors will correspond to state values. The dark squares correspond to walls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the environment\n",
    "np.random.seed(54)\n",
    "env = FourRoomsEnv()\n",
    "# Defining the policy\n",
    "pi = {s:np.random.choice([1,2]) for s in env.states()}\n",
    "# Visualizing the environment and policy\n",
    "zero_values = np.zeros(env.shapes[0])\n",
    "zero_values[env.terminal] = 1\n",
    "env.plot_values_policy(zero_values, pi, plot_all_policy = True, cbar=False)\n",
    "np.random.seed(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. <span style=\"color:blue\"> **TODO** </span> DP Policy Evaluation\n",
    "We want to evaluate $v_\\pi$ for this policy. Note that we could also compute $q_\\pi$, it doesn't really matter if we're not looking to extend to control!\n",
    "\n",
    "We will first use what we know works, i.e., **Dynamic Programming**'s Policy Evaluation, then implement Monte Carlo to compare the results.\n",
    "\n",
    "- <span style=\"color:blue\"> **TODOs** </span>: Implement **DP_PolicyEvaluation**. This will be similar to the Policy and Value Iteration agents from the previous lab, we're providing an MDP as input and implement a `run` method to compute the values. Instead of a dictionary as before, we will now use a **numpy array** to store the $v$ and $q$ from now on, as it is more efficient, more fit for this environment, and easier to visualize. The policy $\\pi$ is a dictionary here, but it makes little practical difference. You can use an In-Place algo, or not, however you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP_PolicyEvaluation():\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "        self.gamma = self.mdp.gamma\n",
    "        self.V = np.zeros(self.mdp.shapes[0])\n",
    "        \n",
    "    def run(self, pi, delta):\n",
    "        \"\"\" Runs Policy evaluation for a given policy. \n",
    "        No returns expected, but use the self.V array to store your values so we can plot it.\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi: dict\n",
    "            Policy to Evaluate. Comes as a dictionary of state:action keys.\n",
    "        delta: float\n",
    "            Precision at which Policy Evaluation should be stopped.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the run method.\n",
    "        raise NotImplementedError(\"run method of DP PolicyEval\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the Policy Evaluation and visualize the estimated values of $\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dp = DP_PolicyEvaluation(env)\n",
    "agent_dp.run(pi, delta=1e-3)\n",
    "env.plot_values_policy(agent_dp.V, pi, plot_all_policy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **TODO** </span>: Shortly **comment** your result.\n",
    "\n",
    "(your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.  <span style=\"color:blue\"> **TODO** </span> Monte Carlo Policy Evaluation\n",
    "\n",
    "We will now find $v_\\pi$ simply by interacting with the environment, using MC. We will need a different `argmax` function from the one in the `numpy` library, because it always returns the first occurence of the max, while we need a random choice of the occurences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing to do in this cell.\n",
    "def allmax(a):\n",
    "    \"\"\" Returns all occurences of the max \"\"\"\n",
    "    if len(a) == 0:\n",
    "        return []\n",
    "    all_ = [0]\n",
    "    max_ = a[0]\n",
    "    for i in range(1, len(a)):\n",
    "        if a[i] > max_:\n",
    "            all_ = [i]\n",
    "            max_ = a[i]\n",
    "        elif a[i] == max_:\n",
    "            all_.append(i)\n",
    "    return all_\n",
    "\n",
    "def my_argmax(v):\n",
    "    \"\"\" Breaks ties randomly. \"\"\"\n",
    "    return random.choice(allmax(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, we are shifting from the Dynamic Programming framework to the more general Reinforcement Learning framework of **Agent-Environment interactions**. Our abstraction for such agents is that they need to be able to `act` given a state, i.e., sample from their policy; and `learn` given a transition $\\left(s,a,r,s^\\prime\\right)$. For practical purposes, we also need to `reset` the agent to $0$ learning, getting ready for another round of learning; similar to the Environment's `reset` method, getting ready for a new episode.\n",
    "\n",
    "- <span style=\"color:red\"> **TODO** </span>: **Remind** why MC Policy Evaluation should converge? <br/> (your answer here)\n",
    "- <span style=\"color:blue\"> **TODO** </span>: **Implement** MC Policy Evaluation provided the policy $\\pi$ we have already evaluated. The policy is now given to the agent as we build it (in `__init__`), since we want the agent to interact using it. Please use `self.V` to store your state value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloPolicyEval():\n",
    "    \"\"\" Given a policy, uses that policy to act in the environment.\n",
    "    Learns its V value.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_shapes, pi, epsilon=0.1, gamma=0.9, **kwargs):\n",
    "        if env_shapes is not None: # otherwise, bandits\n",
    "            self.input_shape, self.n_actions = env_shapes\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.V = np.zeros(self.input_shape) # Array of value estimates.\n",
    "        self.pi = pi\n",
    "        # You can add or remove things in here, if needed\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" \n",
    "        Reset method of your Agent. No inputs, no returns.\n",
    "        \"\"\"\n",
    "        # TODO: Reset your agent. \n",
    "        raise NotImplementedError(\"MonteCarloPolicyEval reset method\")\n",
    "\n",
    "    def act(self, s):\n",
    "        \"\"\" Action in the environment, i.e. sample from our policy.\n",
    "        Parameters\n",
    "        ----------\n",
    "        s: tuple\n",
    "            State from which to give out an action\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Action to perform in the env\n",
    "        \"\"\"\n",
    "        # TODO: act\n",
    "        raise NotImplementedError(\"MonteCarloPolicyEval act method\")\n",
    "        \n",
    "    def learn(self, s, a, r, s_, d=False):\n",
    "        \"\"\" MC learning the policy value from a transition. \n",
    "        Parameters\n",
    "        ----------\n",
    "        s, a, r, s_, d: tuple, int, float, tuple, bool\n",
    "            Transition in the environment.\n",
    "            From state s, taking action a, obtaining reward r and ending up in state s_\n",
    "            d is the done signal, indicating if s_ is terminal.\n",
    "        \"\"\"\n",
    "        # TODO: implement the learning of the MonteCarloPolicyEval agent\n",
    "        raise NotImplementedError(\"MonteCarloPolicyEval act method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement our first true learning loop! Using the [gym](https://gym.openai.com/) framework again for agent-env interactions, here's how we expect a learning loop to look like for a single episode:\n",
    "```\n",
    "state = env.reset()\n",
    "while True:\n",
    "  action = agent.act()\n",
    "  next_state, reward, done, info = env.step(action)\n",
    "  agent.learn(state, action, reward, next_state, done)\n",
    "\n",
    "  if done:\n",
    "    break\n",
    "env.close()\n",
    "```\n",
    "\n",
    "This is what is implemented below, with a bit more to plot things out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning and visualization. Nothing to do here.\n",
    "def learn_v_pi(agent, env, n_episodes):\n",
    "    evaluations_history = []\n",
    "    agent.reset()\n",
    "    first = True\n",
    "    # Training phase\n",
    "    for ep in range(n_episodes):\n",
    "        s = env.reset()\n",
    "        while True: # step limit implemented in Env\n",
    "            action = agent.act(s) # must be = pi[s]\n",
    "            s_, reward, done, info = env.step(action)\n",
    "            agent.learn(s, action, reward, s_, done)\n",
    "            if done:\n",
    "                break\n",
    "            s = s_\n",
    "        if first and np.any(agent.V != 0):\n",
    "            print(\"V visualization after the first rewarding episode observed - episode {}:\".format(ep))\n",
    "            env.plot_values_policy(agent.V, agent.pi, plot_all_policy=True)\n",
    "            first = False\n",
    "        if not first and not ep % 200:\n",
    "            print(\"V visualization after episode {}:\".format(ep))\n",
    "            env.plot_values_policy(agent.V, agent.pi, plot_all_policy=True)\n",
    "        \n",
    "    print(\"Final V visualization: \")\n",
    "    env.plot_values_policy(agent.V, agent.pi, plot_all_policy=True)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we chose a deterministic policy, we actually only need Exploring Starts to go over every single state once in order to get a perfect estimate. However, we will proceed as if $\\pi$ was not deterministic, and without assuming control over the initial states - we know they are random, but no more.\n",
    "\n",
    "We need all states to be sampled. Since there are around $100$ non-terminal states, we can be sure that all will most likely have been selected once if we run $1000$ episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FourRoomsEnv(explo_starts=True, max_steps = 50)\n",
    "shapes = (tuple([env.observation_space[i].n for i in range(len(env.observation_space))]), env.action_space.n)\n",
    "d = {\n",
    "    'env_shapes': shapes,\n",
    "    'epsilon': 0.2,\n",
    "    'gamma': env.gamma,\n",
    "    'pi' : pi\n",
    "}\n",
    "agent_mc = MonteCarloPolicyEval(**d)\n",
    "learn_v_pi(agent_mc, env, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder, to compare with your DP solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_values_policy(agent_dp.V, pi, plot_all_policy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **TODO** </span>: Shortly **compare** your results and the methods.\n",
    "\n",
    "(your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <span style=\"color:green\"> (5 points) </span>  Monte Carlo Control\n",
    "### 2.1. <span style=\"color:blue\"> **TODO** </span> On-Policy MC\n",
    "Policy Evaluation is cool, but the most interesting problem to solve is the Control problem, where we need to find an optimal policy. However, we drastically changed the framework: from perfect access to the MDP dynamics, we now want to learn an optimal policy only from Agent-Environment interactions.\n",
    "\n",
    "- <span style=\"color:red\"> **TODO** </span> **Remind** how MC approaches the Control problem to converge. <br/> (your answer here)\n",
    "- <span style=\"color:red\"> **TODO** </span> **Remind** how on-policy MC works *without exploring starts*.<br/> (your answer here)\n",
    "- <span style=\"color:blue\"> **TODO** </span> **Implement** $\\epsilon$-greedy MC for on-policy Control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGMonteCarlo():\n",
    "    \"\"\" Tabular method that keeps the Q-values of all the possible\n",
    "    state-action pairs; updates on an episode-wise schedule\n",
    "    On-Policy version using epsilon-greedy\n",
    "    \"\"\"\n",
    "    def __init__(self, env_shapes, epsilon=0.1, gamma=0.9, **kwargs):\n",
    "        if env_shapes is not None: # otherwise, bandits\n",
    "            self.input_shape, self.n_actions = env_shapes\n",
    "        self.Q = np.zeros((*self.input_shape, self.n_actions)) # Q value estimates. Access a state with Q[s] and state action pair with Q[s][a]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError(\"EGMonteCarlo reset method\")\n",
    "\n",
    "    def act(self, s):\n",
    "        raise NotImplementedError(\"EGMonteCarlo act method\")\n",
    "\n",
    "    def learn(self, s, a, r, s_, d=False):\n",
    "        raise NotImplementedError(\"EGMonteCarlo reset method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_train_agent(agent, env, n_episodes):\n",
    "    \"\"\" Returns the steps_history of the agent\"\"\"\n",
    "    evaluations_history = []\n",
    "    view = [0,1,3,5,10,100,500]\n",
    "    agent.reset()\n",
    "    # Training phase\n",
    "    steps_history = np.empty(n_episodes)\n",
    "    for ep in range(n_episodes):\n",
    "        t = 0\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            action = agent.act(s)\n",
    "            s_, reward, done, info = env.step(action)\n",
    "            agent.learn(s, action, reward, s_, done)\n",
    "            if done:\n",
    "                break\n",
    "            s = s_\n",
    "            t += 1\n",
    "        if ep in view:\n",
    "            print(\"Best Q and action after episode {}:\".format(ep))\n",
    "            best_qs = np.max(agent.Q, axis=-1)\n",
    "            greedy_policy = np.argmax(agent.Q, axis=-1)\n",
    "            env.plot_values_policy(best_qs, greedy_policy)\n",
    "\n",
    "        steps_history[ep] = t\n",
    "        \n",
    "    \n",
    "    print(\"Final Q and policy:\")\n",
    "    best_qs = np.max(agent.Q, axis=-1)\n",
    "    greedy_policy = np.argmax(agent.Q, axis=-1)\n",
    "    env.plot_values_policy(best_qs, greedy_policy)\n",
    "    env.close()\n",
    "    return steps_history[1:] # First is totally random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is now stochastic exploration, the episodes will all reach the terminal state, if given enough time. We can remove both the step limit and Exploring Starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FourRoomsEnv()\n",
    "shapes = (tuple([env.observation_space[i].n for i in range(len(env.observation_space))]), env.action_space.n)\n",
    "d = {\n",
    "    'env_shapes': shapes,\n",
    "    'epsilon': 0.2,\n",
    "    'gamma': env.gamma,\n",
    "}\n",
    "agent = EGMonteCarlo(**d)\n",
    "perf_EG = view_train_agent(agent, env, 2000)\n",
    "plt.plot(perf_EG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **TODO** </span>: **Explain** your understanding of how MC updates its values.**Comment** the results you obtained and the evolution of the learnt policy.\n",
    "\n",
    "(your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. <span style=\"color:blue\"> **TODO** </span> Off-Policy Monte Carlo\n",
    "On-Policy Monte Carlo without Exploring starts is limited to soft policies. Off-Policy MC offers the promise of learning an optimal policy -and optimal values- by using a behavior policy $b$ different from the learnt policy $\\pi$. In our case, we will follow the course and choose $b$ to be an $\\epsilon$-greedy policy according to our $Q$ estimates, while the learnt $\\pi$ policy is the true optimal policy.\n",
    "- <span style=\"color:blue\"> **TODO** </span> **Implement** MC for Off-policy Control as discussed. Use the same formalism as the On-Policy agent above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffMonteCarloIS(EGMonteCarlo):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FourRoomsEnv()\n",
    "shapes = (tuple([env.observation_space[i].n for i in range(len(env.observation_space))]), env.action_space.n)\n",
    "d = {\n",
    "    'env_shapes': shapes,\n",
    "    'epsilon': 0.2,\n",
    "    'gamma': env.gamma,\n",
    "}\n",
    "agent = OffMonteCarloIS(**d)\n",
    "perf_off = view_train_agent(agent, env, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **TODO** </span>: **Explain** briefly the difference in approach from On to Off policy MC. **Comment** the results you obtained and the evolution of the learnt policy.\n",
    "\n",
    "(your answer here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
